### Design, development & documentation
### Angelina Angelova PhD**, Metagenomics Analysis Specialist, Science Support Section, BCBB/OCICB/OSMO/OD/NIAID/NIH
### Duc Doan**, Software Engineer, Bioinformatics Software Section, BCBB/OCICB/OSMO/OD/NIAID/NIH


configfile: "/usr/local/src/nephele2/pipelines/wgsa/config.yaml"
include: "utils.smk"

TED_READS = config["output_dir"] + "TEDreads/"
TAX_PROFILES = config["output_dir"] + "TAXprofiles/"
if "plusPFV" in config["classification_db"]:
    DBNAME = "RefDB"                                                                                  
elif "MGBCdb" in config["classification_db"]:
    DBNAME = "mgbcDB"
else:
    DBNAME = "EuPathDB"
TED_READS_TAX = TAX_PROFILES + "readsTAX_" + DBNAME +"/"
TED_TMP_NAME = "TED_out/"
TED_TMP = config["output_dir"] + TED_TMP_NAME
TED_FILE_NAME = config["output_dir"] + config["ted_file_name"]
METASPADES = config["output_dir"] + "asmbMetaSpades/"
ASM = METASPADES + "{sample}_asmb/"
OUT_FILE = ASM + "final.assembly"
GENES = ASM + "genes/"
PREGENES_PREFIX = GENES + "PREDgenes"
ANNOTATIONS = GENES + "annotations/"
PATHWAYS = GENES + "pathways/"
PREGENESTAX = GENES + "genesTAX_" + DBNAME + "/"
GENETAX_PREFIX = PREGENESTAX + "geneTAX"
GENETAX = TAX_PROFILES + "geneTAX_" + DBNAME + "/"              ##--> "geneTAX/"  <-- ##
SCAFFTAX = ASM + "scaffTAX_" + DBNAME + "/"                     ##-->
SCAFFTAX_PREFIX = SCAFFTAX + "scaffTAX"                         ##--> "scafftax" <-- ##
MAG = ASM + "MAGs/"
MAG_QA = MAG + "magsQA/"
QC_PLOTS = MAG_QA + "QCplots/"
TAXBIN = TAX_PROFILES + "MAGs_TAX/"
PWY_PROFILES = config["output_dir"] + "PWYprofiles/"

if config["annotations_type"] == "ko":                          ##--> updated conditional arguments
    PWYLABEL="ko2gg"                                            ##--> added, comes in useful later
    ANNO_TYPE="ko"                                              ##--> added, comes in useful later
    PWYDIR = PWY_PROFILES + "keggPWYs.MP/"                      ##--> added .MP for "MinPATH"
else:
    PWYLABEL="ec2mc"                                            ##--> added, comes in useful later
    ANNO_TYPE="ec"                                             ##--> added, comes in useful later
    PWYDIR = PWY_PROFILES + "mtccPWYs.MP/"                      ##--> updated folder name. .MP for "MinPATH"
PATHWAYS_PREFIX = PATHWAYS + PWYLABEL                           ##--> updated
GENEBIN = PWYDIR + "genebin/"                                   ##--> added genebin based on "annot_type"
PWYBIN = PWYDIR + "pwybin/"                                     ##--> added a pwybin (prev just "bin"), for each annot type
PWYPLOTS = PWYDIR + "PWYplots_"+ PWYLABEL + ".html"             ##--> "PWYplots_KEGG.html" ##<-- 
TAXPLOTS = TED_READS_TAX + "TAXplots_readsTAX_" + DBNAME +".html"        ##--> just added for consistency

PWY_MERGED_TABLES = PWYDIR + "merged_tables/"                   ##--> PWY_PROFILES + "merged_tables/" ##<--
TAX_MERGED_TABLES = TED_READS_TAX + "merged_tables/"            ##--> TAX_PROFILES + "merged_tables/" ##<--
PWY_DIVERSITY_PLOTS = PWYDIR + "DivPlots/"                      ##--> PWY_PROFILES + "DiversityPlots/" ##<--
TAX_DIVERSITY_PLOTS = TED_READS_TAX + "DivPlots/"               ##--> TAX_PROFILES + "DiversityPlots/" ##<--
BIOM_FILE_MICROBIOMEDB = config["output_dir"] + config["biom_file_name"]
BIOM_FILE_6C=TAXBIN + "MAG-based_Counts+TAX.biom"
PWYBIOM = PWYDIR + "bioms/"
TAXBIOM = TED_READS_TAX + "bioms/"
TED_READS_TAX_KLOGS = TED_READS_TAX + "klogs/"

samples = read_mapping_file()
threads_per_job = get_num_threads_per_job(len(samples))

pipeline_output = [
    TED_READS_TAX + "TAXplots_readsTAX_" + DBNAME +".html",             ##--> "TAXplots_TEDreads.html",
    expand(ASM + "add_de_replication_stats.done", sample=samples),
    expand(OUT_FILE + "_depths.txt",sample=samples),
    expand(OUT_FILE + "_scaffCoverage.txt",sample=samples),
    expand(PREGENES_PREFIX + "_stats.txt", sample=samples),
    PWYPLOTS,
    expand(PWYBIOM + "{sample}_json.biom", sample=samples),
    expand(TAXBIOM + "{sample}_json.biom", sample=samples),
    expand(ANNOTATIONS + "annots.ko.faa", sample=samples),
    expand(ANNOTATIONS + "annots.ko.fna", sample=samples),
    expand(ANNOTATIONS + "annots.ec.faa", sample=samples),
    expand(ANNOTATIONS + "annots.ec.fna", sample=samples),
    expand(ANNOTATIONS + "{sample}_geneTPMtab.ec.txt", sample=samples),
    expand(ANNOTATIONS + "{sample}_geneTPMtab.ko.txt", sample=samples),
]


# Output for step 6D (AMRFinder; optional)                               ##--> its 6D now <--## 
if config["amrfind"]:
    pipeline_output.extend([
        expand(GENES + "AMRs/AMRs.faa", sample=samples),                 ##-- added "AMRs/" folder <--## 
        expand(GENES + "AMRs/AMRs_info.txt", sample=samples)             ##-- added "AMRs/" folder <--## 
    ])

# Output for step 6A (optional)
if config["tax_genes"]:
    pipeline_output.extend([
        expand(GENETAX_PREFIX + "_ABUNtab+tax.txt", sample=samples),
    ])
    if DBNAME != "mgbcDB":
        pipeline_output.extend([
            GENETAX + "TAXplots_genesTAX_" + DBNAME + ".html"
        ])

# Output for step 6B (optional) 
if config["tax_scaffolds"]:
    pipeline_output.extend([
        expand(SCAFFTAX_PREFIX + "_scaffCovr+tax.txt", sample=samples)   ##--> changed output removed requirement for {_class,noclass.txt}s <--##
    ])

# Output for step 6C (optional)
if config["mag"]:
    pipeline_output.extend([
        expand(MAG_QA + "mag_SUMMARY.txt", sample=samples),
        expand(QC_PLOTS + "checkm_plots.done", sample=samples),
        TAXBIN + "TAXplots_MAGx.html",
        BIOM_FILE_6C
    ])

# Merging binned files from all samples into 1 collated table only run when users provide at least 2 samples
if len(samples) > 1:
    pipeline_output.extend([
        # PWY_MERGED_TABLES + "merged_Counts+PWY+Tiers.txt",
        # PWY_MERGED_TABLES + "merged_Counts+PWY.txt",
        # PWY_MERGED_TABLES + "merged_Counts+PWY_json.biom",
        # PWY_MERGED_TABLES + "merged_Tiers.txt",
        # PWY_MERGED_TABLES + "merged_Counts+Tiers.txt",
        # PWY_MERGED_TABLES + "merged_Counts.txt",
        # PWY_MERGED_TABLES + "merged_PWY.txt",
        # PWY_MERGED_TABLES + "PWY_collation.log",
        # TAX_MERGED_TABLES + "merged_Couns+TAX+Lineage.txt",
        # TAX_MERGED_TABLES + "merged_Counts+TAX.txt",
        # TAX_MERGED_TABLES + "merged_Counts+TAX_json.biom",
        # TAX_MERGED_TABLES + "merged_Lineage.txt",
        # TAX_MERGED_TABLES + "merged_Counts+Lineage.txt",
        # TAX_MERGED_TABLES + "merged_Counts.txt",
        # TAX_MERGED_TABLES + "merged_TAX.txt",
        # TAX_MERGED_TABLES + "TAX_collation.log"
        PWY_MERGED_TABLES + "merged_Counts+allTiers_json.biom",
        TAX_MERGED_TABLES + "merged_Counts+Lineage_json.biom",
        ])

if len(samples) > 1:
    pipeline_output.extend([
        # PWY_DIVERSITY_PLOTS + "PWY_Profile_Heatmap_top35PWYs.pdf",
        # PWY_DIVERSITY_PLOTS + "PWY_BetaDiv_nMDS.pdf",
        # PWY_DIVERSITY_PLOTS + "PWY_BetaDiv_PCoA.pdf",
        # TAX_DIVERSITY_PLOTS + "TAX_AlphaDiv_mapping.txt",
        # TAX_DIVERSITY_PLOTS + "TAX_AlphaDiv.pdf",
        # TAX_DIVERSITY_PLOTS + "TAX_BetaDiv_PCoA.pdf",
        # TAX_DIVERSITY_PLOTS + "TAX_BetaDiv_nMDS.pdf",
        # TAX_DIVERSITY_PLOTS + "TAX_Profile_Heatmap.pdf",
        # TAX_DIVERSITY_PLOTS + "TAX_RareficaitonCurve.pdf",
        # TAX_DIVERSITY_PLOTS + "TAX_RankAbundanceCurve.pdf"
        PWY_DIVERSITY_PLOTS + "PWY_diversity_plots.done",
        TAX_DIVERSITY_PLOTS + "TAX_diversity_plots.done"
    ])

onstart:
    shell(
    """
    set +o pipefail
    echo "Dependencies:"
    fastp --version
    python3 {config[metaspades_script]} --version
    bowtie2 --version | awk '{{print "bowtie2 version", $3}}' | head -n1
    samtools --version | head -n1
    metabat2 2>&1 >/dev/null | grep version
    prodigal -v 2>&1 >/dev/null | head -n2 | tail -n1
    checkm | head -n2 | tail -n1 | sed 's/^ *//g'
    ktImportText | head -n2 | tail -n1 | sed 's/[^a-zA-Z0-9. ]//g' | sed 's/^ *//g'
    metaprokka --version
    cat {config[minpath_script]} | grep version | head -n1
    verse -v | head -n2 | tail -n1
    kraken2 --version | head -n1
    amrfinder --version | awk '{{print "AMRFinder version", $1}}'
    emapper.py --version | tail -n1 | awk '{{print $1}}'
    R --version | head -n1
    snakemake --version | awk '{{print "Snakemake version", $1}}'
    """)

rule all:
    input:
        pipeline_output,

# Step 2 starts
rule run_fastp:
    input:
        unpack(get_fastq_gz)
    threads: threads_per_job
    output:
        f=temp(TED_READS + "{sample}_R1_te.fastq.gz"),
        r=temp(TED_READS + "{sample}_R2_te.fastq.gz"),
        s_l_html=TED_READS + "{sample}_fastplog.html",
        s_l_json=temp(TED_READS + "{sample}_fastp.json"),                       ## no change, correct as is
        s_l_text=TED_READS + "{sample}_fastplog.txt",                           ##--> added to cmd() but maybe not needed <--##
    run:
        shell(run_fastp_cmd())                                                 ##--> added to cmd def (in utils) code to collect the ouptut log for each sample <--##


rule run_kraken2_decontaminate:
    resources:
        mem_gb=config["total_mem_gb"]
    input:
        f=rules.run_fastp.output.f,
        r=rules.run_fastp.output.r
    threads: threads_per_job
    params:
        # Can not put this in output because it will cause missing files. kraken2 requires to have the symbol # for output.
        out=TED_READS + "{sample}_R#_ted.fastq",
        decontaminate=config["decontaminate"]
    output:
        unclassified_out1=temp(TED_READS + "{sample}_R_1_ted.fastq"),
        unclassified_out2=temp(TED_READS + "{sample}_R_2_ted.fastq"),
        out=temp(TED_READS + "{sample}_kr2outLOG.txt"),                                 ##--> gets rm after rule
        report=TED_READS + "{sample}_kr2_decontamREPORT.txt",
        outlog=TED_READS + "{sample}_kr2_decontamLOG.txt"                               ##--> this can be kept & reported
    shell:
        "kraken2 --use-names --gzip-compressed --threads {threads} "
        "--confidence 0.05 --db {params.decontaminate} "
        "--paired {input.f} {input.r} "
        "--unclassified-out {params.out} "
        "--output {output.out} "
        "--report {output.report} 2>> {output.outlog}"

 ##--> removed the "rename_kraken2_decotnaminate rule" cuz not needed anymore

rule compress_kraken2_decontaminate:                                     
    input:
        f=rules.run_kraken2_decontaminate.output.unclassified_out1,
        r=rules.run_kraken2_decontaminate.output.unclassified_out2
    output:
        f=TED_TMP + "{sample}_R1_ted.fastq.gz",                          ##--> updated the expected output <--##
        r=TED_TMP + "{sample}_R2_ted.fastq.gz"
    shell:
        "pigz -c {input.f} > {output.f} &&"                 ##--> added the pigz -c > filename option"  <--##
        "pigz -c {input.r} > {output.r}"                    ##--> added the pigz -c > filename option"  <--##


# Step 3 starts
rule run_kraken2_classification:
    resources:
            mem_gb=config["total_mem_gb"]
    input:
        f=rules.compress_kraken2_decontaminate.output.f,
        r=rules.compress_kraken2_decontaminate.output.r,
    threads: threads_per_job
    params:
        classification_db=config["classification_db"]
    output:
        out=temp(TED_READS_TAX_KLOGS + "{sample}_klog.txt"),
        report=TED_READS_TAX + "reports/{sample}_taxREPORT.txt",
        outlog=TED_READS_TAX_KLOGS + "{sample}_classLOG.txt"                         ##--> adding a classLOG.txt file
    shell:
        "kraken2 --use-names --gzip-compressed --threads {threads} "
        "--confidence 0.05 --db {params.classification_db} --memory-mapping "
        "--paired {input.f} {input.r} "
        "--output {output.out} "
        "--report {output.report} 2>> {output.outlog}"                              ##--> adding a classLOG capture file

rule kreport2krona_per_sample:
    input:
        krfile=rules.run_kraken2_classification.output.report
    params:
        kreport2krona_script=config["kreport2krona_script"],
    output:
        binfile=TED_READS_TAX + "bin/{sample}_wTAXid_4krona.txt",                                                                           ##--> modified code
    shell:
        "python3 {params.kreport2krona_script} --report-file {input.krfile} --no-tax-prefixes -o {output.binfile} &&"                                  ##--> updated  code <--## 
        "sed -i '2s/\\t\\t/\\tUnclassified\\t/g' {output.binfile} &&"                  ## needed twice                                 ##--> added  code , no output <--## 
        "sed -i '2s/\\t\\t/\\tUnclassified\\t/g' {output.binfile}"
 ##--> removing the table merging capapcity of the kreport2krona_per_sample rule, since it overprints the bin/files (required output)
 ##--> removed the mod_kreport2krona_output rule cuz mods are above and no output <--## 


rule kreport2krona_import_text:
    input:
        expand(rules.kreport2krona_per_sample.output.binfile, sample=samples)
    output:
        TAXPLOTS                                                                 ##--> updated name + in pipline_output
    shell:
        "ktImportText {input} -o {output}"

# Step 4 starts
rule run_metaspades:
    input:
        in1=rules.compress_kraken2_decontaminate.output.f,
        in2=rules.compress_kraken2_decontaminate.output.r
    params:
        out=ASM,
        metaspades_script=config["metaspades_script"],
        tmp_dir=config["tmp_dir"]
    threads: threads_per_job
    output:
        temp(ASM + "scaffolds.fasta")
    shell:
        "python3 {params.metaspades_script} -1 {input.in1} -2 {input.in2} "
        "--memory 550 --tmp-dir {params.tmp_dir} "
        "-o {params.out} -t {threads} --only-assembler"

rule run_bbtools_stats:
    input:
        rules.run_metaspades.output
    params:
        asm=ASM,
        bbtools_stats=config["bbtools_stats"]
    output:
        out1=OUT_FILE + ".fasta",
        out2=OUT_FILE + "_stats.txt"
    shell:
        "sed $'s/_cov_/ cov_/g' {input} > {output.out1} && "
        "{params.bbtools_stats} in={output.out1} out={output.out2} && "
        "printf \"\\nTEDreads mapping stats:\\n\"  >> {output.out2} && "
        "rm -rf {params.asm}K* {params.asm}pipeline_state {params.asm}tmp {params.asm}misc {params.asm}before_rr.fasta "
        "{params.asm}dataset.info {params.asm}*params* {params.asm}*.gfa {params.asm}first_pe_contigs.fasta "
        "{params.asm}run_spades.sh {params.asm}*.yaml {params.asm}*.paths {params.asm}assembly_graph.fastg"

# Step 5 starts
rule bowtie2_build:
    input:
        rules.run_bbtools_stats.output.out1
    params:
        basename=ASM + "{sample}.db"
    threads: threads_per_job
    output:
        # to signal this step is completed
        temp(ASM + "bowtie2_build.done")
    shell:
       "bowtie2-build --quiet --threads {threads} {input} {params.basename} && touch {output}"

rule bowtie2_run:
    input:
        in1=rules.compress_kraken2_decontaminate.output.f,
        in2=rules.compress_kraken2_decontaminate.output.r,
        in3=rules.bowtie2_build.output, # fake input so it can run after bowtie2_build
    params:
        index=ASM + "{sample}.db",
        in4=rules.run_bbtools_stats.output.out2 # if in4 is in input, it will cause this step to re-run all the time (due to input changes)
    threads: threads_per_job
    output:
        sam=temp(ASM + "{sample}.sam")
        # sam=ASM + "{sample}.sam"
    shell:
        "bowtie2 --phred33 --sensitive-local --no-unal --seed 4 "
        "-1 {input.in1} -2 {input.in2} "
        "-x {params.index} "
        "-S {output.sam} "
        "-p {threads} 2>> {params.in4}"

rule samtools_to_bam:
    input:
        rules.bowtie2_run.output.sam
    params:
        collate_tmp=config["tmp_dir"] + "{sample}_collate",
        sort_tmp=config["tmp_dir"] + "{sample}_sort",
        markdup_tmp=config["tmp_dir"] + "{sample}_markdup"
    threads: threads_per_job
    output:
        bam=temp(ASM + "{sample}.bam"),
        bam_colated=temp(ASM + "{sample}_colated.bam"),
        bam_fixmate=temp(ASM + "{sample}_fixmate.bam"),
        bam_fixmate_sorted=temp(ASM + "{sample}_fixmate_sorted.bam"),
        tmp_markdup=temp(ASM + "{sample}_tmp_markdup.txt"),
        final_bam=OUT_FILE + ".bam"
    shell:
        "samtools sort {input} -o {output.bam} -@ {threads} -T {params.sort_tmp} && "
        "samtools collate {output.bam} -o {output.bam_colated} -@ {threads} {params.collate_tmp} && "
        "samtools fixmate -m {output.bam_colated} {output.bam_fixmate} -@ {threads} && "
        "samtools sort {output.bam_fixmate} -o {output.bam_fixmate_sorted} -@ {threads} -T {params.sort_tmp} && "
        "samtools markdup -r -s {output.bam_fixmate_sorted} -f {output.tmp_markdup} {output.final_bam} -@ {threads} -T {params.markdup_tmp} && "
        "samtools index -b {output.final_bam} -@ {threads}"

rule add_de_replication_stats:
    input:
        in1=rules.run_bbtools_stats.output.out2,
        in2=rules.samtools_to_bam.output.tmp_markdup
    params:
        stats_file=OUT_FILE + "_stats.txt",
        tmp_out=ASM + "{sample}_tmp_file.txt"
    output:
        # to signal this step is completed
        ASM + "add_de_replication_stats.done"
    shell:
        "cat {input.in1} <(printf \"\\nRead alignment de-replication stats:\\n\") {input.in2} > {params.tmp_out} && "
        "mv {params.tmp_out} {params.stats_file} && touch {output}"

rule scaffold_and_read_counts:
    input:
        final_bam=rules.samtools_to_bam.output.final_bam
    params:
        bbtools_pileup=config["bbtools_pileup"]
    output:
        tmp_scaffcov=temp(ASM + "tmp_scaffcov.txt"),
        tmp_basecov=temp(ASM + "tmp_basecov.txt"),
        tmp_idx=temp(ASM + "tmp_idx.txt"),
        tmp_readcount=temp(ASM + "tmp_readcount.txt"),
        depths=OUT_FILE + "_depths.txt",
        tmp_scaffcov1=temp(ASM + "tmp_scaffcov1.txt"),                                                                                                         ##--> added tmp file <--##
        scaffCoverage=OUT_FILE + "_scaffCoverage.txt",
    shell:
        "{params.bbtools_pileup} {input.final_bam} out={output.tmp_scaffcov} overwrite=true && "
        "sort -o {output.tmp_scaffcov} {output.tmp_scaffcov} && "
        "jgi_summarize_bam_contig_depths {input.final_bam} --outputDepth {output.depths} && "
        "sort {output.depths} | sed '1 s/^/#/' | cut -f 1-3 > {output.tmp_basecov} && "
        "samtools idxstats {input.final_bam} > {output.tmp_idx} && "
        "cut -f 1-3 {output.tmp_idx} | sed -e '1s/^/#contigName\\tLength\\tReadsCount\\n/' | sort | grep -v \"*\" > {output.tmp_readcount} && "
        "paste {output.tmp_readcount} {output.tmp_scaffcov} {output.tmp_basecov} | cut -f 1,2,3,8-14,17 -d $'\\t' > {output.tmp_scaffcov1} && "                      ##--> changed output to new tmp <--##
        "awk 'BEGIN{{FS=OFS=\"\\t\"}} NR>1 {{if ($2>0 && $3>0) $12=sprintf(\"%0.2f\", $3*1e3/$2); else $12=0; print}}'  {output.tmp_scaffcov1} | "                   ##--> added RPK code <--
        "sed -e '1s/^/#NODE\\tlen\\tReads\\t\%ScaffCovered\\t\%bpCovered\\tplusReads\\tminusReads\\t\%GC\\tMedFold\\tstDev\\tAveDepth\\tRPK\\n/' > {output.scaffCoverage}"     ##--> added RPK code <--

# Step 6 starts
rule gene_prediction:
    input:
        rules.run_bbtools_stats.output.out1
    threads: threads_per_job
    output:
        gff=PREGENES_PREFIX + ".gff",
        faa=PREGENES_PREFIX + ".faa",
        fna=PREGENES_PREFIX + ".fna"
    shell:
        "prodigal -p meta -i {input} -f gff -o {output.gff} -a {output.faa} -d {output.fna} -q"

rule gff_to_gtf:
    input:
        rules.gene_prediction.output.gff
    params:
        gff2gtf_script=config["gff2gtf_script"]
    output:
        PREGENES_PREFIX + ".gtf"
    shell:
        "{params.gff2gtf_script} {input} > {output}"

rule run_verse:
    input:
        gtf=rules.gff_to_gtf.output,
        final_bam=rules.samtools_to_bam.output.final_bam
    threads: threads_per_job
    params:
        basename=GENES + "verse"
    output:
        out1=GENES + "verse.CDS.summary.txt",
        out2=temp(GENES + "verse.CDS.txt")
    shell:
        "verse -a {input.gtf} -t 'CDS' -g gene_id -z 0 -s 0 "
        "-o {params.basename} "
        "{input.final_bam} -T 4"

rule rename_verse_summary:
    input:
        rules.run_verse.output.out1
    output:
        PREGENES_PREFIX + "_stats.txt"
    shell:
        "mv {input} {output}"

rule tpm_normalization:
    input:
        gtf=rules.gff_to_gtf.output,
        verse_cds=rules.run_verse.output.out2,
    output:
        tmp_lengths=temp(GENES + "tmp_lengths.txt"),
        tmp_counts=temp(GENES + "tmp_counts.txt"),
        tmp_coverage=temp(GENES + "tmp_coverage.txt"),
        tmp_rpk=temp(GENES + "tmp_RPK.txt"),
        ABUNtab=PREGENES_PREFIX + "_ABUNtab.txt",
    shell:
        "cut -f4,5,9 {input.gtf} | sed 's/gene_id //g' | gawk '{{print $3,$2-$1+1}}' | tr ' ' '\\t' | sed '1s/^/gene\\tlength\\n/' > {output.tmp_lengths} && "
        "join {output.tmp_lengths} {input.verse_cds} -t $'\\t' > {output.tmp_counts} && "
        "grep \"_\" {output.tmp_counts}   | awk -v OFS=\"\\t\" '{{$4 = sprintf(\"%0.0f\", $3*150/$2)}}1' | sort | sed -e '1s/^/#name\\tlen\\treads\\tcov\\n/' > {output.tmp_coverage} && "                              ##--> do we even need this? <--##
        "grep \"_\" {output.tmp_coverage} | awk -v OFS=\"\\t\" '{{$5 = sprintf(\"%0.0f\", $3*1e3/$2)}}1' | sort | sed -e '1s/^/#name\\tlen\\treads\\tcov\\tRPK\\n/' > {output.tmp_rpk} && "                             ##--> updatad code <--
        "awk -v OFS=\"\\t\" 'NR==FNR{{sum+= $5; next}} FNR==1{{print $0,\"iTPM\"; next}} {{printf(\"%s %0.0f\\n\", $0,$5*1e6/sum)}}'  {output.tmp_rpk} {output.tmp_rpk} |sed -e 's/\\s/\\t/g' > {output.ABUNtab}  "     ##--> calc iTPM now <--

# Step 6A starts
if config["tax_genes"]:
    rule tax_classification_genes:
        resources:
            mem_gb=config["total_mem_gb"]
        input:
            fna=rules.gene_prediction.output.fna
        threads: threads_per_job
        params:
            classification_db=config["classification_db"]
        output:
            klog=temp(GENETAX_PREFIX + "_klog.txt"),
            report=GENETAX_PREFIX + "_taxREPORT.txt",
            outlog=GENETAX_PREFIX + "_classLOG.txt"                         ##--> added outlog <--##
            # classified_out=GENETAX_PREFIX + "_classified.fna",            ##--> removed <--##
            # unclassified_out=GENETAX_PREFIX + "_unclassified.fna"         ##--> removed <--##
        shell:
            "kraken2 --use-names --threads {threads}  --db {params.classification_db} "
            "--confidence 0.05 {input.fna} --memory-mapping "
            "--output {output.klog} "
            "--report {output.report} 2>> {output.outlog}"                  ##--> added outlog <--##
            # "--classified-out {output.classified_out} "                   ##--> removed <--##
            # "--unclassified-out {output.unclassified_out}"                ##--> removed <--##

    rule tax_genes_ABUNtabtax:                                                    ##--> added new rule
        input:
            klog=rules.tax_classification_genes.output.klog,
            ABUNtab=rules.tpm_normalization.output.ABUNtab,
        output:
            tmptax=temp(GENETAX_PREFIX + "_tmptax.txt"),
            ABUNtax=GENETAX_PREFIX + "_ABUNtab+tax.txt",
        shell:
            "cut -f2-3 {input.klog} | sed -e 's/ (taxid /\\t/g' | sed -e 's/)$//g' | sort | sed '1s/^/#contigName\\tTAXname\\tTAXid\\n/' > {output.tmptax} &&"           ##--> added
            "paste {input.ABUNtab} <(cut -f2-3 {output.tmptax}) > {output.ABUNtax} "

    if DBNAME == "mgbcDB":                                             ##-->PROBLEM: creating this condition but not sure of sintax
        rule omit_krona:                                                                    ##--> added new rule
            output:
                done=temp(GENETAX_PREFIX + "omittingKrona.done")
            shell:
                """
                print("---> krona plots omitted for selected classification DB")
                touch {output.done}
                """
    else:
        rule tax_genes_4krona:                                                              ##--> updating rule
            input:
                ABUNtax=rules.tax_genes_ABUNtabtax.output.ABUNtax,                          ##--> now using the ABUNDtab+tax.txt file directly
            output:
                out_4krona=GENETAX + "bin/{sample}_4krona.txt"
            shell:
                "cut -f6-8 {input.ABUNtax} | awk -F'\\t' 'NR>1{{k = $3; sum[k]+= $1; name[k]=$2}} END {{ print; for (k in sum) print sum[k], name[k], k}}' OFS=\"\t\" |"
                "sed -e '1 s/^/#iTPM\\tTAXname\\tTAXid\\n/' > {output.out_4krona}"              ##--> new code, now outputs directly in TAXprofiles/geneTAX/bin

        rule kt_tax_html:
            input:
                expand(rules.tax_genes_4krona.output.out_4krona, sample=samples)
            params:
                krona_tax_db=config["krona_tax_db"]
            output:
                GENETAX + "TAXplots_genesTAX_" + DBNAME + ".html"                                              ##--> updated name of fle
            shell:
                "ktImportTaxonomy {input} -m 1 -q 2 -t 3 -d 10 -tax {params.krona_tax_db} -o {output}"          ##--> just the settings updated


# Step 6B starts
if config["tax_scaffolds"]:
    rule tax_classification_scaffolds:
        resources:
            mem_gb=config["total_mem_gb"]
        input:
            rules.run_bbtools_stats.output.out1
        threads: threads_per_job
        params:
            classification_db=config["classification_db"]
        output:
            klog=temp(SCAFFTAX_PREFIX + "_klog.txt"),
            report=SCAFFTAX_PREFIX + "_taxREPORT.txt",
            outlog=SCAFFTAX_PREFIX + "classLOG.txt",
            # classified=SCAFFTAX_PREFIX + "_classified.fna",               ##--> removed <--##
            # unclassified=SCAFFTAX_PREFIX + "_unclassified.fna"            ##--> removed <--##
        shell:
            "kraken2 --use-names --db {params.classification_db} --confidence 0.05 --memory-mapping "
            "--threads {threads} {input} "
            "--output {output.klog} "
            "--report {output.report} 2>> {output.outlog}"
            # "--classified-out {output.classified} "                              ##--> removed <--##
            # "--unclassified-out {output.unclassified}"                           ##--> removed <--##

    rule generate_scafftax:
        input:
            klog=rules.tax_classification_scaffolds.output.klog,             ##--> named <--##
            # scov=OUT_FILE + "_scaffCoverage.txt",                            ##--> added <--##
            scov=rules.scaffold_and_read_counts.output.scaffCoverage,
        output:
            scovtax=SCAFFTAX_PREFIX + "_scaffCovr+tax.txt",                   ##--> named <--##
            tmp=temp(SCAFFTAX_PREFIX + "tmp_tax.txt"),                        ##--> added <--## 
            # classified=SCAFFTAX_PREFIX + "_classified.txt",                ##--> removed <--##
            # unclassified=SCAFFTAX_PREFIX + "_unclassified.txt"             ##--> removed <--##    
        shell:
            "cut -f2-3 {input.klog} | sed -e 's/ (taxid /\\t/g' | sed -e 's/)$//g' | sort | "         ##--> code changed
            "sed '1s/^/#contigName\\tTAXname\\tTAXid\\n/' > {output.tmp} && "
            "paste {input.scov} <(cut -f2-3 {output.tmp}) > {output.scovtax}"                         ##<-- will not be continuing to htmls <--##


# Step 6C starts
if config["mag"]:
    rule metabat2_run:
        input:
            fasta=rules.run_bbtools_stats.output.out1,
            depths=rules.scaffold_and_read_counts.output.depths
        threads: threads_per_job
        params:
            basename=MAG + "mag"
        output:
            # MAG + "metabat2_run.done"
            temp(MAG + "metabat2_run.done")
        shell:
            "metabat2 -i {input.fasta} -o {params.basename} -m 1500 --maxEdges 250 "
            "--unbinned  -t {threads} -a {input.depths} && touch {output}"

    rule checkm_lineage_wf:
        input:
            # We do not use the actual output here but need files from metabat2_run step
            rules.metabat2_run.output
        params:
            ftype="fa",
            basename=MAG,
            out=MAG_QA
        threads: threads_per_job
        output:
            MAG_QA + "lineage.ms"
        shell:
            "checkm lineage_wf --tab_table --nt --pplacer_threads {threads} "
            "-t {threads} -x {params.ftype} {params.basename} {params.out}"

    rule checkm_qa:
        input:
            rules.checkm_lineage_wf.output
        params:
            mag_qa=MAG_QA
        threads: threads_per_job
        output:
            qa_txt=MAG_QA + "mag_qa.txt",
            tax=MAG_QA + "mag_tax.txt"
        shell:
            "checkm qa -o 2 --tab_table -t {threads} -f {output.qa_txt} {input} {params.mag_qa} && "
            "checkm tree_qa -o 2 --tab_table -f {output.tax} {params.mag_qa}"

    # Too much crap logs, using 2>/dev/null
    rule checkm_coverage:
        input:
            bam=rules.samtools_to_bam.output.final_bam,
            fake=rules.metabat2_run.output # We do not use the actual output here but need files from metabat2_run step
        params:
            ftype="fa",
            mag=MAG
        threads: threads_per_job
        output:
            MAG_QA + "mag_coverage.txt"
        shell:
            "checkm coverage -x {params.ftype} -t {threads} {params.mag} {output} {input.bam} 2>/dev/null"

    rule checkm_profile:
        input:
            rules.checkm_coverage.output
        output:
            MAG_QA + "mag_profiles.txt"
        shell:
            "checkm profile -q {input} --tab_table -f {output}"

    rule join_profiles_tax:
        input:
            tax=rules.checkm_qa.output.tax,
            qa_txt=rules.checkm_qa.output.qa_txt,
            profiles=rules.checkm_profile.output
        output:
            tmp1=temp(MAG_QA + "tmp1.txt"),
            tmp2=temp(MAG_QA + "tmp2.txt"),
            tmpsum=temp(MAG_QA + "mag_tmpsum.txt"),
            summary=MAG_QA + "mag_SUMMARY.txt"
        shell:
            "cut {input.tax} -f1,5 > {output.tmp1} && "
            "cut {input.qa_txt} -f1,6-11,13,15,17,19,23 > {output.tmp2} && "
            "paste {output.tmp1} {output.tmp2} > {output.tmpsum} && "
            "join {input.profiles} {output.tmpsum} -t $'\\t' | cut -f 1,3,4,6,7-19 | sed 's/final.assembly: //g' > {output.summary}"

    rule checkm_plots:
        input:
            in1=rules.metabat2_run.output, # need for MAG
            in2=rules.checkm_lineage_wf.output # need for MAG_QA
        params:
            ftype="fa",
            mag=MAG,
            mag_qa=MAG_QA,
            out=QC_PLOTS
        output:
            # Something to signal this rule is completed
            QC_PLOTS + "checkm_plots.done"
        shell:
            """
            checkm coding_plot --image_type pdf  -x {params.ftype} {params.mag_qa} {params.mag} {params.out} 95 || true
            checkm marker_plot --image_type pdf  -x {params.ftype} --dpi 400 {params.mag_qa} {params.mag} {params.out} || true
            checkm nx_plot --image_type pdf  -x {params.ftype} {params.mag} {params.out} || true
            touch {output}
            """

    rule taxbin_krona_txt:
        input:
            profiles=rules.checkm_profile.output,
            tax=rules.checkm_qa.output.tax
        output:
            TAXBIN + "bin/{sample}_4krona.txt"
        shell:
            "join {input.profiles} {input.tax} -t $'\\t' | cut -f6,10,11 | sed -e 's/..__/\\t/g' | sed -e 's/\\tunresolved//g' | sed -e '1 s/^/#/' > {output}"

    rule taxbin_krona_html:
        input:
            expand(rules.taxbin_krona_txt.output, sample=samples)
        output:
            TAXBIN + "TAXplots_MAGx.html"
        shell:
            "ktImportText {input} -o {output}"

    rule make_biom:
        input:
            profiles=expand(rules.checkm_profile.output, sample=samples),
            tax=expand(rules.checkm_qa.output.tax, sample=samples)
        params:
            map_file=config["map_file"],
            TAXCOL = "Taxonomy (contained)",
            PROFCOL = "final.assembly: mapped reads"
        output:
            BIOM_FILE_6C
        message: """rule {rule}
        Making biom file from '{params.TAXCOL}' column in output of checkm tree_qa,
        '{params.PROFCOL}' column in checkm profiles, and metadata.
        metadata: {params.map_file}
        input: {input}
        output: {output}
        jobid: {jobid}"""
        run:
            make_biom_file(str(rules.checkm_profile.output),
                       str(rules.checkm_qa.output.tax),
                       samples.keys(),
                       params.map_file,
                       str(output), params.TAXCOL, params.PROFCOL)

# Step 6D                                                                                                                                       ##--> updated code condition <--##
if config["amrfind"]:
    rule run_amrfinder:
        input:
            rules.gene_prediction.output.faa
        threads: threads_per_job
        output:
            out1=GENES + "AMRs/AMRs.faa",
            out2=GENES + "AMRs/AMRs_info.txt",
        run:
            print("Running AMRFinder")
            shell("amrfinder --plus --protein {input} --threads {threads} --protein_output {output.out1} -o {output.out2}")


# Step 7 starts
rule gene_annotation:
    resources:
        mem_gb=config["total_mem_gb"] # make sure not run out of memory
    input:
        rules.gene_prediction.output.faa
    threads: threads_per_job
    params:
        eggnog_db=config["eggnog_db"],
        out_dir=ANNOTATIONS,
        tmp_dir=config["tmp_dir"]
    output:
        emapper_hits=ANNOTATIONS + "annots.emapper.hits",
        emapper_seed_orthologs=ANNOTATIONS + "annots.emapper.seed_orthologs",
        emapper_annotations=ANNOTATIONS + "annots.emapper.annotations",
    shell:
        "emapper.py -i {input} -o annots --output_dir {params.out_dir} --cpu {threads} "
        "--itype proteins --block_size 4 --index_chunks 2 --data_dir {params.eggnog_db} "
        "--override --temp_dir {params.tmp_dir}  --scratch_dir {params.tmp_dir} --dbmem"

rule gene_annotation_grep:
    input:
        rules.gene_annotation.output.emapper_annotations
    output:
        ec=ANNOTATIONS + "annots.ec.txt",
        cog=ANNOTATIONS + "annots.COG.txt",
        ko=ANNOTATIONS + "annots.ko.txt",
        keggmap=ANNOTATIONS + "annots.KEGGmap.txt"
    shell:
        """
        grep \"NODE_\" {input} | cut -f 1,11 | sed -e 's/\\t-//g' | grep -e $'\\t' | awk '{{n=split($2,s,\",\");for (i=1;i<=n;i++) {{$2=s[i];print}}}}' | sed -e 's/\\s/\\t/g' > {output.ec} || true
        grep \"NODE_\" {input} | cut -f 1,5 > {output.cog} || true
        grep \"NODE_\" {input} | cut -f1,12 | grep \"ko:\" | sed 's/ko://g' | awk '{{n=split($2,s,\",\");for (i=1;i<=n;i++) {{$2=s[i];print}}}}' | sed -e 's/\\s/\\t/g' > {output.ko} || true
        grep \"NODE_\" {input} | cut -f1,13 | grep \"ko\" | sed 's/,map.*//g' | sed 's/ko/map/g' | awk '{{n=split($2,s,\",\");for (i=1;i<=n;i++) {{$2=s[i];print}}}}' | sed -e 's/\\s/\\t/g' > {output.keggmap} || true
        """
 
rule gene_annot_seqExtract:                                                                       ##--> added rule, but seqtk will be needed <--##
    input: 
        faa=rules.gene_prediction.output.faa,
        fna=rules.gene_prediction.output.fna,
        kos=rules.gene_annotation_grep.output.ko,
        ecs=rules.gene_annotation_grep.output.ec,
    output:
        kofaa=ANNOTATIONS + "annots.ko.faa",
        kofna=ANNOTATIONS + "annots.ko.fna",
        ecfaa=ANNOTATIONS + "annots.ec.faa",
        ecfna=ANNOTATIONS + "annots.ec.fna",
    shell:
        """
        seqtk subseq {input.faa} {input.kos} > {output.kofaa}
        seqtk subseq {input.fna} {input.kos} > {output.kofna}
        seqtk subseq {input.faa} {input.ecs} > {output.ecfaa}
        seqtk subseq {input.fna} {input.ecs} > {output.ecfna}
        """
rule gene_annot_iTPMs:                                                                                           ##--> rule added to calc iTPMs per gene instance
    input:
        abufile=rules.tpm_normalization.output.ABUNtab, 
        grp_kos=rules.gene_annotation_grep.output.ko,
        grp_ecs=rules.gene_annotation_grep.output.ec,
    output:
        abukos=ANNOTATIONS + "ANNOTgenes_ABUNtab.ko.txt",
        abuecs=ANNOTATIONS + "ANNOTgenes_ABUNtab.ec.txt",
    shell:
        """
        awk -v OFS=\"\\t\" 'NR==FNR {{ id[$1]=$0; next }} ($1 in id){{ print $2, id[$1]}}'  {input.abufile} {input.grp_kos} | sed '1s/^/#KO\\tnodeID\\tlen\\treads\\tcov\\tiRPK\\tiTPM\\n/' > {output.abukos}
        awk -v OFS=\"\\t\" 'NR==FNR {{ id[$1]=$0; next }} ($1 in id){{ print $2, id[$1]}}'  {input.abufile} {input.grp_ecs} | sed '1s/^/#KO\\tnodeID\\tlen\\treads\\tcov\\tiRPK\\tiTPM\\n/' > {output.abuecs}        
        """

rule gene_annot_geneTPMs:                                                                                       ##--> fancy new rule added 
    input:
        abukos=rules.gene_annot_iTPMs.output.abukos,
        abuecs=rules.gene_annot_iTPMs.output.abuecs,
    params:
        awk_geneTPM_script=config["awk_geneTPM_script"],
        awk_joinTPM_script=config["awk_joinTPM_script"],
        KO_list=config["KO_listfile"],
        EC_list=config["EC_listfile"]
    output:
        tmpko=temp(ANNOTATIONS + "tmp_geneTPMtab.ko.txt"),
        TPMko=ANNOTATIONS + "{sample}_geneTPMtab.ko.txt",
        tmpec=temp(ANNOTATIONS + "tmp_geneTPMtab.ec.txt"),
        TPMec=ANNOTATIONS + "{sample}_geneTPMtab.ec.txt",
    shell:
        """
        cut -f1,6 {input.abukos} | sed $'s/#KO.*//g' | awk -F'\\t' -f {params.awk_geneTPM_script} > {output.tmpko}
        awk -F'\\t' -f {params.awk_joinTPM_script} {params.KO_list} {output.tmpko} | \
        sed '1s/^/#KO\\tgeneTPM\\tgeneNAME\\n/'> {output.TPMko}

        cut -f1,6 {input.abuecs} | sed $'s/#EC.*//g' | awk -F'\\t' -f {params.awk_geneTPM_script} > {output.tmpec}
        awk -F'\\t' -f {params.awk_joinTPM_script} {params.EC_list} {output.tmpec} | \
        sed '1s/^/#EC\\tgeneTPM\\tgeneNAME\\n/'> {output.TPMec}
        """
        ## the output needs to be cp to PWYdir/genebin downstream?

rule cp_geneTPM_files:                          ##--> this rule is from step 8, but snakemake allows it here
    input: 
        kos=expand(rules.gene_annot_geneTPMs.output.TPMko, sample=samples),
        ecs=expand(rules.gene_annot_geneTPMs.output.TPMec, sample=samples),
    output:
        expand(GENEBIN + "{sample}_geneTPMtab." + ANNO_TYPE + ".txt", sample=samples)
        # if config["annotations_type"] == "ko":
        #     expand(GENEBIN + "{sample}_geneTPMtab.ko.txt", sample=samples)
        # else:
        #     expand(GENEBIN + "{sample}_geneTPMtab.ec.txt", sample=samples) 
    params:
        out_dir=GENEBIN,
        # type_ano=config["annotations_type"],
    run:
        if config["annotations_type"] == "ko":
            shell("cp {input.kos} {params.out_dir}")
        else:
            shell("cp {input.ecs} {params.out_dir}")


# Step 8 starts
rule copy_annotation_file:
    input:
        ko=rules.gene_annotation_grep.output.ko,
        ec=rules.gene_annotation_grep.output.ec,
        ABUNtab=rules.tpm_normalization.output.ABUNtab
    output:
        tmp_annots=temp(PATHWAYS + "tmp_annots.txt"),
        tmp_TPM=temp(PATHWAYS + "tmp_TPM.txt"),
        # tmp_annots=PATHWAYS + "tmp_annots.txt",
        # tmp_TPM=PATHWAYS + "tmp_TPM.txt"
    run:
        if config["annotations_type"] == "ko":
            shell("cp {input.ko} {output.tmp_annots} && cut -f 1,5 {input.ABUNtab} > {output.tmp_TPM}")
        else:
            shell("cp {input.ec} {output.tmp_annots} && cut -f 1,5 {input.ABUNtab} > {output.tmp_TPM}")

rule run_min_path:
    input:
        rules.copy_annotation_file.output.tmp_annots
    params:
        minpath_script=config["minpath_script"],
        annotations_map=config["annotations_map"],
    output:
        tmp_report=temp(PATHWAYS + "tmp_report.txt"),
        details=PATHWAYS_PREFIX + ".details.txt",
        minpath_log=PATHWAYS_PREFIX + ".minpath.log.txt",
    shell:
        """
        if [ -s {input} ]; then
            python3 {params.minpath_script} -any {input} -map {params.annotations_map} -report {output.tmp_report} -details {output.details} >> {output.minpath_log}
        else
            echo "The input file {input} is empty, so run_min_path will not run. All output file(s) from this step are empty."
            touch {output.tmp_report} {output.details} {output.minpath_log}
        fi
        """
        # "python3 {params.minpath_script} -any {input} -map {params.annotations_map} "
        # "-report {output.tmp_report} -details {output.details} >> {output.minpath_log}"

rule create_pathways_report:
    input:
        rules.run_min_path.output.tmp_report
    output:
        PATHWAYS_PREFIX + ".report.txt"
    shell:
        """
        if [ -s {input} ]; then
            grep 'minpath\ 1' {input} > {output}
        else
            echo "The input file {input} is empty, so create_pathways_report will not run. All output file(s) from this step are empty."
            touch {output}
        fi
        """
        # "grep 'minpath\ 1' {input} > {output}"

rule run_genes2krona:
    input:
        tmp_annots=rules.copy_annotation_file.output.tmp_annots,
        tmp_TPM=rules.copy_annotation_file.output.tmp_TPM,
        report=rules.create_pathways_report.output
    params:
        krona_script=config["genes2krona_script"],
        annotations_map=config["annotations_map"],
        annotations_hrr=config["annotations_hrr"]
    output:
        tmp_4kr=temp(PATHWAYS + "tmp_4kr.txt"),
        krona_out=PATHWAYS + "{sample}_4krona.txt",
        krona_out_clone=PWYBIN + "{sample}_4krona.txt",
    shell:
        """
        if [ -s {input.tmp_annots} -a -s {input.tmp_TPM} -a -s {input.report} ]; then
            python3 {params.krona_script} -i {input.tmp_annots}\
            -m {params.annotations_map} -H {params.annotations_hrr} -n {wildcards.sample} -c {input.tmp_TPM}\
            -l {input.report} -o {output.tmp_4kr} &&\
            sed '1s/^/#/' {output.tmp_4kr} > {output.krona_out} &&\
            cp {output.krona_out} {output.krona_out_clone}
        else
            echo "One of the input file is empty, so run_genes2krona will not run. All output file(s) from this step are empty."
            touch {output.tmp_4kr} {output.krona_out} {output.krona_out_clone}
        fi
        """

rule krona_import_text:
    input:
        expand(rules.run_genes2krona.output.krona_out, sample=samples)
    output:
        PWYPLOTS
    shell:
        "ktImportText {input} -o {output}"


# Step 9 starts
if len(samples) > 1:
    rule TAX_collation:                                                                                     ##--> updated rule
        input:
            expand(rules.kreport2krona_per_sample.output.binfile, sample=samples)                           ##--> updated input
        params:
            bin_dir=TED_READS_TAX + "bin/",
            gen_dir="NA",                                                                                    ##--> added
            string="_wTAXid_4krona.txt",                                                                     ##--> added
            tax_table_merging_script=config["tax_table_merging_script"],                                     ##--> updated
            tmp_dir=config["tmp_dir"],
            out_dir=TAX_MERGED_TABLES
        output:
            scclist=temp(TED_READS_TAX + "SccList.txt"),
            out1=TAX_MERGED_TABLES + "merged_Counts+TAX+Lineage.txt",
            out2=TAX_MERGED_TABLES + "merged_Counts+TAX.txt",
            out3=TAX_MERGED_TABLES + "merged_Lineage.txt",
            out4=TAX_MERGED_TABLES + "merged_Counts+Lineage.txt",
            out5=TAX_MERGED_TABLES + "merged_Counts.txt",
            out6=TAX_MERGED_TABLES + "merged_TAX.txt",
            logR=TAX_MERGED_TABLES + "TAX_collationR.log"
        shell:
            "find {params.bin_dir}/*_4krona.txt -type f -empty -delete && "
            "ls -1 {params.bin_dir} | xargs basename -a -s {params.string} > {output.scclist} && "
            "Rscript {params.tax_table_merging_script} --binDIR {params.bin_dir} --sccList {output.scclist} --outdir {params.out_dir} --genesDIR {params.gen_dir} 2>&1 >{output.logR}"

    rule PWY_collation:                                                                                  ##--> updated rule
        input:
            pwyfiles=expand(rules.run_genes2krona.output.krona_out_clone, sample=samples),
            # gnefiles=expand(GENEBIN + "{sample}_geneTPMtab." + ANNO_TYPE + ".txt", sample=samples),        ##--> added
            gnefiles=expand(rules.cp_geneTPM_files.output, sample=samples),
        params:
            bin_dir=PWYBIN,                                                                               ##--> updated
            gen_dir=GENEBIN,
            # string="_" + PWYLABEL,                                                                        ##--> added
            string="_4krona.txt", 
            pwy_table_merging_script=config["pwy_table_merging_script"],                                  ##--> updated script
            tmp_dir=config["tmp_dir"],
            out_dir=PWY_MERGED_TABLES
        output:
            scclist=temp(PWYDIR + "SccList.txt"),
            out1=PWY_MERGED_TABLES + "merged_Counts+PWY+allTiers.txt",
            out2=PWY_MERGED_TABLES + "merged_Counts+PWY.txt",
            out3=PWY_MERGED_TABLES + "merged_allTiers.txt",
            out4=PWY_MERGED_TABLES + "merged_Counts+allTiers.txt",
            out5=PWY_MERGED_TABLES + "merged_Counts.txt",
            out6=PWY_MERGED_TABLES + "merged_PWY.txt",
            out7=PWY_MERGED_TABLES + "merged_geneTPMtable.txt",                                             ##--> added output <--##
            logR=PWY_MERGED_TABLES + "PWY_collation.log"
        shell:                                                                                              ##--> updated R input
            "find {params.bin_dir}/*.txt -type f -empty -delete && "
            "find {params.gen_dir}/*.txt -type f -empty -delete && "
            "ls -1 {params.bin_dir} | xargs basename -a -s {params.string} > {output.scclist} && head {output.scclist} &&"
            "Rscript {params.pwy_table_merging_script} --binDIR {params.bin_dir} --sccList {output.scclist} --outdir {params.out_dir} --genesDIR {params.gen_dir} 2>&1 >{output.logR}"
    

# step 10                                              ##--> biom files are now their own step to be run after step 9 <--##
rule tax_biom_per_sample: 
    input:
        rules.kreport2krona_per_sample.output.binfile
    params:
        tabtype="OTU table"
    output:
        txt=temp(TAXBIOM + "{sample}_4biom.txt"),
        biom=TAXBIOM + "{sample}_json.biom"
    shell:
        "sed 's/\\t/;/g' {input} | sed 's/;/\\t/1' | nl -n ln |sed '1s/^/\\t/' | sed '1s/1\\s\\+\\t#/\\t/' | sed 's/\\s\\+\\t/\\t/g' | sed '1s/LINEAGE/Taxonomy/' | sed -e '1s/Level.*/Taxonomy/' > {output.txt} && "
        "biom convert -i {output.txt} -o {output.biom} --to-json --table-type='{params.tabtype}' --process-obs-metadata taxonomy"

rule pwy_biom_per_sample:
    input:
        rules.run_genes2krona.output.krona_out_clone
    params:
        tabtype="Pathway table"
    output:
        txt=temp(PWYBIOM + "{sample}_4biom.txt"),
        biom=PWYBIOM + "{sample}_json.biom"
    shell:
        """
        if [ -s {input} ]; then
            sed 's/\\t/;/g' {input} | sed 's/;/\\t/1' | nl -n ln |sed '1s/^/\\t/' | sed '1s/1\\s\\+\\t#/\\t/' | sed 's/\\s\\+\\t/\\t/g' | sed '1s/LINEAGE/Taxonomy/' | sed -e '1s/Level.*/Taxonomy/' > {output.txt}
            biom convert -i {output.txt} -o {output.biom} --to-json --table-type='{params.tabtype}' --process-obs-metadata taxonomy || touch {output.biom}
        else
            echo "The input file {input} is empty, so pwy_biom_per_sample will not run. All output file(s) from this step are empty."
            touch {output.txt} {output.biom}
        fi
        """

if len(samples) > 1:
    rule TAX_collation_biom:
        input:
            tax_collation=rules.TAX_collation.output.out4,
            mapping_file=config["map_file"]
        params:
            tabtype="OTU table"
        output:
            biom=TAX_MERGED_TABLES + "merged_Counts+Lineage_json.biom",
            tmp=temp(TAX_MERGED_TABLES + "tmp_.txt")
        shell:
            "sed '1s/Lineage/Taxonomy/' {input.tax_collation} > {output.tmp} && "
            "biom convert -i {output.tmp} -m <(sed '1s/^/#/' {input.mapping_file}) -o {output.biom} --to-json --table-type='{params.tabtype}' --process-obs-metadata taxonomy"
    
    rule PWY_collation_biom:
        input:
            pwy_collation=rules.PWY_collation.output.out4,
            mapping_file=config["map_file"]
        params:
            tabtype="Pathway table"
        output:
            biom=PWY_MERGED_TABLES + "merged_Counts+allTiers_json.biom",                ##--> updated name
            tmp=temp(PWY_MERGED_TABLES + "tmp_.txt")
        shell:
            "sed '1s/allTiers/Taxonomy/' {input.pwy_collation} > {output.tmp} && "
            "biom convert -i {output.tmp} -m <(sed '1s/^/#/' {input.mapping_file}) -o {output.biom} --to-json --table-type='{params.tabtype}' --process-obs-metadata taxonomy"


# Step 11 starts                                            ##--> now step 11
# Ignore if fail: "because those are statistics and whether they fail or not depend on the actual dataset, not on the performance of the pipeline"
if len(samples) > 1:
    rule TAX_diversity_plots:
        input:
            in1=rules.TAX_collation.output.out2,
            in2=rules.TAX_collation.output.out5,
            in3=rules.TAX_collation.output.out6,
            mapping_file=config["map_file"]
        params:
            merged_tables_dir=TAX_MERGED_TABLES,
            tax_diversity_plots_script=config["tax_diversity_plots_script"],
            tmp_dir=config["tmp_dir"],
            out_dir=TAX_DIVERSITY_PLOTS
        output:
            # out1=TAX_DIVERSITY_PLOTS + "TAX_AlphaDiv_mapping.txt",
            # out2=TAX_DIVERSITY_PLOTS + "TAX_AlphaDiv.pdf",
            # out3=TAX_DIVERSITY_PLOTS + "TAX_BetaDiv_PCoA.pdf",
            # out4=TAX_DIVERSITY_PLOTS + "TAX_BetaDiv_nMDS.pdf",
            # out5=TAX_DIVERSITY_PLOTS + "TAX_Profile_Heatmap.pdf",
            # out6=TAX_DIVERSITY_PLOTS + "TAX_RareficaitonCurve.pdf",
            # out7=TAX_DIVERSITY_PLOTS + "TAX_RankAbundanceCurve.pdf",
            tmp_mapping_file=temp(TAX_DIVERSITY_PLOTS + "tmp_mapping_file.txt"),
            done=TAX_DIVERSITY_PLOTS + "TAX_diversity_plots.done"
        shell:
            """
            sed 's/^#//g' {input.mapping_file} > {output.tmp_mapping_file}
            Rscript {params.tax_diversity_plots_script} --wdir {params.tmp_dir} --indir {params.merged_tables_dir} --mfile {output.tmp_mapping_file} --outdir {params.out_dir} || echo "ERROR: Failed to generate TAX_diversity_plots"
            touch {output.done}
            """
  
    rule PWY_diversity_plots:
        input:
            in1=rules.PWY_collation.output.out2,
            in2=rules.PWY_collation.output.out5,
            in3=rules.PWY_collation.output.out6,
            mapping_file=config["map_file"]
        params:
            merged_tables_dir=PWY_MERGED_TABLES,
            pwy_diversity_plots_script=config["pwy_diversity_plots_script"],
            tmp_dir=config["tmp_dir"],
            out_dir=PWY_DIVERSITY_PLOTS
        output:
            # out1=PWY_DIVERSITY_PLOTS + "PWY_Profile_Heatmap_top35PWYs.pdf",
            # out2=PWY_DIVERSITY_PLOTS + "PWY_BetaDiv_nMDS.pdf",
            # out3=PWY_DIVERSITY_PLOTS + "PWY_BetaDiv_PCoA.pdf",
            tmp_mapping_file=temp(PWY_DIVERSITY_PLOTS + "tmp_mapping_file.txt"),
            done=PWY_DIVERSITY_PLOTS + "PWY_diversity_plots.done"
        shell:
            """
            sed 's/^#//g' {input.mapping_file} > {output.tmp_mapping_file}
            Rscript {params.pwy_diversity_plots_script} --wdir {params.tmp_dir} --indir {params.merged_tables_dir} --mfile {output.tmp_mapping_file} --outdir {params.out_dir} || echo "ERROR: Failed to generate PWY_diversity_plots"
            touch {output.done}
            """
 

# onsuccess:
#     shell(clean_output() + handle_TED() + handle_cp_biom())

# onerror:
#     shell(clean_output() + handle_TED() + handle_cp_biom())